# MNIST_Classification_Model-using-CNN ðŸ¤–ðŸ§ ðŸ‡¦ðŸ‡®ðŸ‘¾
_A Deep Learning Project Comparing Convoluted Neural Network (CNN) vs. Artificial Neural Network (ANN) Performance after being trained on the same dataset that undergone the same data cleaning and transformation_
<br><br><br>

## ðŸ“Œ Problem Statement
The MNIST dataset (Modified National Institute of Standards and Technology) is a classic benchmark in machine learning, consisting of 70,000 handwritten digit images (0-9). While simple for modern deep learning models, it remains a fundamental test for evaluating neural network architectures.

Key Challenges: <br>
âœ” High accuracy is achievable, but model efficiency and generalization matter.<br>
âœ” Comparing Convolutional Neural Networks (CNN) vs. Artificial Neural Networks (ANN) helps understand which architecture performs better for image classification. <br>
âœ” Optimizing hyperparameters to minimize overfitting while maximizing accuracy.<br>
<br><br><br>
## ðŸŽ¯ Objectives
âœ” Build and train a CNN model for MNIST digit classification.<br>
âœ” Compare performance between CNN and ANN to determine which architecture is better suited for image recognition.<br>
âœ” Achieve high accuracy (>99%) while avoiding overfitting.<br>
âœ” Visualize model predictions and misclassifications to understand limitations.<br>
<br><br><br>

## ðŸ“Š Dataset Overview
- Source: Classical MNIST dataset.
- Size: 70,000 grayscale images (28x28 pixels).
- Split: <br>
   âœ” 60,000 training images<br>
   âœ” 10,000 test images<br>
- Classes: 10 (digits 0-9). <br>
#### ðŸ”— Dataset Reference: [Yann LeCunâ€™s MNIST Page](http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz)
<br><br><br>

## ðŸ›  Skills & Technologies Used
### ðŸ“Œ Programming & Tools
- Python (Primary Language)
- Jupyter Notebook (Interactive Development)
- Git & GitHub (Version Control)

### ðŸ“Œ Key Libraries
- Deep Learning:	PyTorch
- Data Handling:	NumPy, Pandas
- Visualization:	Matplotlib, Seaborn
- Model Evaluation:	Scikit-learn (Metrics)
### ðŸ“Œ Model Architectures Tested
 **Convolutional Neural Network (CNN)**
- Conv2D + MaxPooling layers
- Dropout for regularization
- Softmax output
- ReLU activation
<br><br><br>

## âš  Limitations & Challenges
1. Simple Dataset: MNIST is relatively easy; real-world handwritten digits may vary more.
2. Overfitting Risk: Without proper regularization, models may memorize training data.
3. Computational Cost: CNNs are slower to train than ANNs on simple images.
4. Generalization: Performance on different handwriting styles needs testing.
<br><br><br>

## ðŸš€ Future Improvements
âœ… Data Augmentation: Rotations, shifts, and noise to improve robustness.<br>
âœ… Advanced Architectures: Using Transfer Learning to train leverage on Advaced Models such as ResNet, EfficientNet and Vgg16_model for comparison.<br>
âœ… Deployment: Convert model to TensorFlow Lite for mobile/edge devices.<br>
âœ… Real-World Testing: Apply to custom handwritten digit images.<br>
<br><br><br>

## ðŸ“œ License
This project is open-source under the MIT License. Contributions & feedback welcome!
<br><br><br>

 
 ## ðŸ“¬ Open to collaboration
 You can  create a pull request with detailed explanation if you would love to work more on this, or contact me through:
 - [Github](https://www.github.com/Abdulbasit4422).
 - [LinkedIn](https://www.linkedin.com/in/oyetunjiabdulbasitoyebamiji)
 - [X](https://mobile.x.com/Abdulbasitoyeb1)
 - [Facebook](https://www.facebook.com/abdulbasit.oyetunji?mibextid=ZbWKwL)
 - Gmail --> abdulbasitoyetunji88@gmail.com






